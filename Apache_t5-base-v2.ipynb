{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-15T12:27:11.398241Z",
     "start_time": "2024-08-15T12:26:56.092830Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Veri Yükleme\n",
    "data = pd.read_csv('data/apache/data.csv')\n",
    "\n",
    "# Veri çerçevesindeki sütun adlarını kontrol et\n",
    "print(\"Sütun Adları:\", data.columns)\n",
    "\n",
    "# Tarih sütununu datetime formatına dönüştürme\n",
    "data['Date'] = pd.to_datetime(data['Date'].str.strip('[]'), format='%d/%b/%Y:%H:%M:%S %z', errors='coerce')\n",
    "\n",
    "# Eksik Verilerin Kontrolü\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"Eksik Veriler:\", missing_values)\n",
    "\n",
    "# Gereksiz Sütunları Kaldırma (eğer veri çerçevesinde varsa)\n",
    "data = data.drop(columns=['Referrer', 'User Agent'], errors='ignore')\n",
    "\n",
    "# Yeni Özellikler Ekleme: Yıl, Ay, Gün, Saat, Dakika\n",
    "data['Year'] = data['Date'].dt.year\n",
    "data['Month'] = data['Date'].dt.month\n",
    "data['Day'] = data['Date'].dt.day\n",
    "data['Hour'] = data['Date'].dt.hour\n",
    "data['Minute'] = data['Date'].dt.minute\n",
    "\n",
    "# 'Status Code' sütununu sayısal değerlere dönüştürme (eğer gerekliyse)\n",
    "data['Status Code'] = pd.to_numeric(data['Status Code'], errors='coerce')\n",
    "\n",
    "# Boş veya Hatalı Verileri Temizleme\n",
    "data = data.dropna(subset=['Request', 'Endpoint', 'Status Code'])\n",
    "data = data[data['Request'].str.strip() != '']\n",
    "data = data[data['Endpoint'].str.strip() != '']\n",
    "data = data[data['Status Code'].notna()]  # Sayısal sütunları kontrol et\n",
    "\n",
    "# Birleştirilen metin verisini oluştur\n",
    "data['Combined'] = data['Request'] + ' ' + data['Endpoint'] + ' ' + data['Status Code'].astype(str)\n",
    "\n",
    "# Boş verileri kontrol et\n",
    "print(data['Combined'].head())\n",
    "print(data['Combined'].isnull().sum())\n",
    "\n",
    "# TF-IDF Vektörizasyonu\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data['Combined']).toarray()\n",
    "\n",
    "# FAISS İndeksi Oluşturma ve Vektörlerin Eklenmesi\n",
    "index = faiss.IndexFlatL2(X.shape[1])\n",
    "index.add(X.astype('float32'))\n",
    "\n",
    "# Kullanıcı Sorgusunu Vektörleştirme ve En Yakın Komşuları Bulma\n",
    "def find_relevant_logs(query):\n",
    "    query_vector = vectorizer.transform([query]).toarray().astype('float32')\n",
    "    D, I = index.search(query_vector, 5)\n",
    "    return data.iloc[I[0]]\n",
    "\n",
    "# GPT-2 Modeli ile Yanıt Oluşturma\n",
    "def generate_response(logs):\n",
    "    input_text = \" \".join(\n",
    "        logs.apply(lambda row: f\"{row['Date']} {row['Request']} {row['Endpoint']} {row['Status Code']}\",\n",
    "                   axis=1).tolist())\n",
    "    input_text = input_text[:1000]  # Giriş metnini 1000 karakterle sınırlama\n",
    "\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
    "\n",
    "    output = model.generate(input_ids, attention_mask=attention_mask, max_length=150, num_return_sequences=1,\n",
    "                            pad_token_id=tokenizer.eos_token_id)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Sorgu Yanıt Sistemi\n",
    "def answer_question(query):\n",
    "    relevant_logs = find_relevant_logs(query)\n",
    "    response = generate_response(relevant_logs)\n",
    "    return response\n",
    "\n",
    "# Model ve Tokenizer Yükleme\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sütun Adları: Index(['IP Address', 'Date', 'Request', 'Endpoint', 'Status Code',\n",
      "       'Response Size', 'Referrer', 'User Agent', 'Time Taken'],\n",
      "      dtype='object')\n",
      "Eksik Veriler: IP Address       0\n",
      "Date             0\n",
      "Request          0\n",
      "Endpoint         0\n",
      "Status Code      0\n",
      "Response Size    0\n",
      "Referrer         0\n",
      "User Agent       0\n",
      "Time Taken       0\n",
      "dtype: int64\n",
      "0                 GET /usr/admin 502\n",
      "1                POST /usr/login 500\n",
      "2    DELETE /usr/admin/developer 200\n",
      "3       PUT /usr/admin/developer 200\n",
      "4           DELETE /usr/register 502\n",
      "Name: Combined, dtype: object\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reyiz\\anaconda3\\envs\\NLP\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-15T12:27:35.124082Z",
     "start_time": "2024-08-15T12:27:33.507120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Test Sorgusu\n",
    "query = \"Son 24 saatte hangi URL'ler 200 hatası aldı?\"  # Buraya sorgunuzu girin\n",
    "response = answer_question(query)\n",
    "print(\"Modelin Yanıtı:\", response)"
   ],
   "id": "f05866b8334ca78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelin Yanıtı: 2019-02-23 10:21:21+03:00 DELETE /usr 200 2019-05-02 03:05:48+03:00 DELETE /usr 200 2018-08-27 10:40:48+03:00 DELETE /usr 200 2019-04-06 06:07:04+03:00 DELETE /usr 200 2018-07-09 04:59:51+03:00 DELETE /usr 200 2018-07-09 04:59:51+03:00 DELETE /usr 200 2018-07-09 04:59:51+03:00 DELETE /usr 200 2018-07-09 04:59:51\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fd1e8d3e760101b2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
