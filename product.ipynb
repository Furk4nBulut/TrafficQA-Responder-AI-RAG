{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-18T23:02:20.307801Z",
     "start_time": "2024-08-18T23:02:20.304539Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from data_generator import DataGenerator"
   ],
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T23:03:41.326164Z",
     "start_time": "2024-08-18T23:02:20.635151Z"
    }
   },
   "cell_type": "code",
   "source": "DataGenerator.main()",
   "id": "28367f3fb70c9ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dosyası başarıyla oluşturuldu!\n",
      "Test dosyası oluşturuldu!\n",
      "Log dosyası başarıyla CSV formatına dönüştürüldü!\n",
      "Log dosyası başarıyla CSV formatına dönüştürüldü!\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T22:58:47.946587Z",
     "start_time": "2024-08-18T22:58:46.292529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv('data/apache/data.csv')\n",
    "print(\"Sütun Adları:\", data.columns)"
   ],
   "id": "2c1c8cece7726f48",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sütun Adları: Index(['IP Address', 'Date', 'Request', 'Endpoint', 'Status Code',\n",
      "       'Response Size', 'Referrer', 'User Agent', 'Time Taken'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T22:59:01.344041Z",
     "start_time": "2024-08-18T22:58:51.851779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data['Date'] = pd.to_datetime(data['Date'].str.strip('[]'), format='%d/%b/%Y:%H:%M:%S %z', errors='coerce')\n",
    "\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"Eksik Veriler:\", missing_values)\n",
    "\n",
    "data = data.drop(columns=['Referrer', 'User Agent'], errors='ignore')\n",
    "\n",
    "data['Year'] = data['Date'].dt.year\n",
    "data['Month'] = data['Date'].dt.month\n",
    "data['Day'] = data['Date'].dt.day\n",
    "data['Hour'] = data['Date'].dt.hour\n",
    "data['Minute'] = data['Date'].dt.minute\n",
    "\n",
    "data['Status Code'] = pd.to_numeric(data['Status Code'], errors='coerce')\n",
    "\n",
    "data = data.dropna(subset=['Request', 'Endpoint', 'Status Code'])\n",
    "data = data[data['Request'].str.strip() != '']\n",
    "data = data[data['Endpoint'].str.strip() != '']\n",
    "data = data[data['Status Code'].notna()] \n",
    "\n",
    "data['Combined'] = data['Request'] + ' ' + data['Endpoint'] + ' ' + data['Status Code'].astype(str)\n",
    "\n",
    "print(data['Combined'].head())\n",
    "print(data['Combined'].isnull().sum())\n"
   ],
   "id": "d3bbf1445a08b748",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eksik Veriler: IP Address       0\n",
      "Date             0\n",
      "Request          0\n",
      "Endpoint         0\n",
      "Status Code      0\n",
      "Response Size    0\n",
      "Referrer         0\n",
      "User Agent       0\n",
      "Time Taken       0\n",
      "dtype: int64\n",
      "0    DELETE /usr/admin 500\n",
      "1       GET /usr/login 403\n",
      "2    GET /usr/register 303\n",
      "3            POST /usr 502\n",
      "4    PUT /usr/register 403\n",
      "Name: Combined, dtype: object\n",
      "0\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T22:59:05.352992Z",
     "start_time": "2024-08-18T22:59:02.182305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(data['Combined']).toarray()\n",
    "\n",
    "index = faiss.IndexFlatL2(X.shape[1])\n",
    "index.add(X.astype('float32'))\n"
   ],
   "id": "64676fa066921727",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T22:59:21.154658Z",
     "start_time": "2024-08-18T22:59:21.148737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_relevant_logs(query):\n",
    "    query_vector = vectorizer.transform([query]).toarray().astype('float32')\n",
    "    D, I = index.search(query_vector, 5)\n",
    "    return data.iloc[I[0]]\n",
    "\n",
    "def generate_response(logs):\n",
    "    input_text = \" \".join(\n",
    "           logs.apply(\n",
    "        lambda row: f\"{row['IP Address']} {row['Date']} {row['Request']} {row['Endpoint']} {row['Status Code']} \"\n",
    "                    f\"{row['Response Size']}{row['Time Taken']}\",\n",
    "        axis=1\n",
    "    ).tolist()\n",
    "    )\n",
    "    input_text = input_text[:1000]  # Giriş metnini 1000 karakterle sınırlama\n",
    "\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
    "\n",
    "    output = model.generate(input_ids, attention_mask=attention_mask, max_length=150, num_return_sequences=1,\n",
    "                            pad_token_id=tokenizer.eos_token_id)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "def answer_question(query):\n",
    "    relevant_logs = find_relevant_logs(query)\n",
    "    response = generate_response(relevant_logs)\n",
    "    return response"
   ],
   "id": "6fe8c10b250c17a5",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T22:59:22.454817Z",
     "start_time": "2024-08-18T22:59:21.717660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ],
   "id": "38b9e60d0b08f3bf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reyiz\\anaconda3\\envs\\NLP\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T22:59:22.537115Z",
     "start_time": "2024-08-18T22:59:22.533819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test Sorguları Listesi\n",
    "queries = [\n",
    "    \"Son 24 saatte hangi URL'ler 500 hatası aldı?\",\n",
    "    \"Son bir ayda hangi IP adresleri en fazla 403 hatası aldı?\",\n",
    "    \"Son bir yıl içinde en sık kullanılan POST isteklerinin listesi nedir?\",\n",
    "    \"Son 30 gün içinde hangi tarayıcılar en fazla 404 hatası aldı?\",\n",
    "    \"Son haftada hangi endpoint'ler en yüksek Response Size'a sahipti?\",\n",
    "    \"En son 10 istekte hangi User Agent'lar kullanıldı?\",\n",
    "    \"En yüksek zaman alımı (Time Taken) olan 5 istek nedir?\",\n",
    "    \"Son 6 ayda hangi Referrer en çok ziyaret edildi?\",\n",
    "    \"Son 24 saatte hangi Endpoint'lerde 502 hatası alındı?\",\n",
    "    \"Hangi IP adresleri en uzun süre GET isteği yaptı?\",\n",
    "    \"Which IP adress has Longest GET time ?\",\n",
    "]"
   ],
   "id": "f05866b8334ca78",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T22:59:25.151062Z",
     "start_time": "2024-08-18T22:59:23.368295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Her bir sorguyu test etme\n",
    "for query in queries:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    response = answer_question(query)\n",
    "    print(f\"Sorgu: {query}\")\n",
    "    print(f\"Modelin Yanıtı: {response}\")\n",
    "    print(\"\\n\")\n"
   ],
   "id": "fd1e8d3e760101b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Sorgu: Son 24 saatte hangi URL'ler 500 hatası aldı?\n",
      "Modelin Yanıtı: 111.139.229.63 2018-05-15 02:12:55+03:00 PUT /usr 500 49994871 40.147.32.105 2019-09-28 02:11:01+03:00 PUT /usr 500 50053221 187.238.79.103 2019-06-20 04:45:46+03:00 PUT /usr 500 49843327 10.209.39.248 2019-06-15 07:53:20+03:00 PUT /usr 500 49741237 188.98.72.174 2019-04-24 06:04:50+03:00 PUT /usr 500 50801447 188.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Sorgu: Son bir ayda hangi IP adresleri en fazla 403 hatası aldı?\n",
      "Modelin Yanıtı: 50.229.180.34 2018-06-26 07:10:06+03:00 PUT /usr 403 49764292 139.35.27.252 2019-09-18 11:19:21+03:00 PUT /usr 403 50153575 6.218.243.113 2018-03-26 10:51:23+03:00 PUT /usr 403 4952579 177.32.56.190 2018-12-04 08:03:54+03:00 PUT /usr 403 49804008 214.196.158.77 2019-08-24 08:32:29+03:00 PUT /usr 403 49923748 6.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Sorgu: Son bir yıl içinde en sık kullanılan POST isteklerinin listesi nedir?\n",
      "Modelin Yanıtı: 2.101.249.34 2018-12-01 12:11:45+03:00 POST /usr 403 50373593 67.15.114.63 2019-06-06 12:07:03+03:00 POST /usr 403 5081382 41.70.93.160 2019-03-24 02:45:44+03:00 POST /usr 403 50193263 86.208.121.221 2019-08-22 01:52:48+03:00 POST /usr 403 4963671 209.149.21.135 2018-11-13 11:16:11+03:00 POST /usr 403 5046444 69.79.198.20\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Sorgu: Son 30 gün içinde hangi tarayıcılar en fazla 404 hatası aldı?\n",
      "Modelin Yanıtı: 154.155.12.230 2018-01-05 08:41:24+03:00 PUT /usr 404 48592227 5.43.114.160 2019-04-19 08:59:22+03:00 PUT /usr 404 50314318 204.139.10.34 2019-01-27 01:34:57+03:00 PUT /usr 404 51131552 94.37.8.255 2018-11-18 07:00:29+03:00 PUT /usr 404 49634898 185.134.215.184 2019-01-10 10:43:54+03:00 PUT /usr 404 49742001 5.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input length of input_ids is 158, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[69], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m80\u001B[39m)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 6\u001B[0m response \u001B[38;5;241m=\u001B[39m answer_question(query)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSorgu: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquery\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModelin Yanıtı: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[66], line 26\u001B[0m, in \u001B[0;36manswer_question\u001B[1;34m(query)\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21manswer_question\u001B[39m(query):\n\u001B[0;32m     25\u001B[0m     relevant_logs \u001B[38;5;241m=\u001B[39m find_relevant_logs(query)\n\u001B[1;32m---> 26\u001B[0m     response \u001B[38;5;241m=\u001B[39m generate_response(relevant_logs)\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "Cell \u001B[1;32mIn[66], line 19\u001B[0m, in \u001B[0;36mgenerate_response\u001B[1;34m(logs)\u001B[0m\n\u001B[0;32m     16\u001B[0m input_ids \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mencode(input_text, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     17\u001B[0m attention_mask \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mones(input_ids\u001B[38;5;241m.\u001B[39mshape, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong)\n\u001B[1;32m---> 19\u001B[0m output \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgenerate(input_ids, attention_mask\u001B[38;5;241m=\u001B[39mattention_mask, max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m150\u001B[39m, num_return_sequences\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m     20\u001B[0m                         pad_token_id\u001B[38;5;241m=\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39meos_token_id)\n\u001B[0;32m     21\u001B[0m response \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mdecode(output[\u001B[38;5;241m0\u001B[39m], skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\transformers\\generation\\utils.py:1874\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[0;32m   1867\u001B[0m         model_kwargs[cache_name] \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   1868\u001B[0m             DynamicCache\u001B[38;5;241m.\u001B[39mfrom_legacy_cache(past)\n\u001B[0;32m   1869\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m requires_cross_attention_cache\n\u001B[0;32m   1870\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m EncoderDecoderCache\u001B[38;5;241m.\u001B[39mfrom_legacy_cache(past)\n\u001B[0;32m   1871\u001B[0m         )\n\u001B[0;32m   1872\u001B[0m         use_dynamic_cache_by_default \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m-> 1874\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n\u001B[0;32m   1876\u001B[0m \u001B[38;5;66;03m# 7. determine generation mode\u001B[39;00m\n\u001B[0;32m   1877\u001B[0m generation_mode \u001B[38;5;241m=\u001B[39m generation_config\u001B[38;5;241m.\u001B[39mget_generation_mode(assistant_model)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\transformers\\generation\\utils.py:1266\u001B[0m, in \u001B[0;36mGenerationMixin._validate_generated_length\u001B[1;34m(self, generation_config, input_ids_length, has_default_max_length)\u001B[0m\n\u001B[0;32m   1264\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m input_ids_length \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m generation_config\u001B[38;5;241m.\u001B[39mmax_length:\n\u001B[0;32m   1265\u001B[0m     input_ids_string \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdecoder_input_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1266\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1267\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput length of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minput_ids_string\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minput_ids_length\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, but `max_length` is set to\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1268\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgeneration_config\u001B[38;5;241m.\u001B[39mmax_length\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. This can lead to unexpected behavior. You should consider\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1269\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m increasing `max_length` or, better yet, setting `max_new_tokens`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1270\u001B[0m     )\n\u001B[0;32m   1272\u001B[0m \u001B[38;5;66;03m# 2. Min length warnings due to unfeasible parameter combinations\u001B[39;00m\n\u001B[0;32m   1273\u001B[0m min_length_error_suffix \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   1274\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Generation will stop at the defined maximum length. You should decrease the minimum length and/or \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1275\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mincrease the maximum length.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1276\u001B[0m )\n",
      "\u001B[1;31mValueError\u001B[0m: Input length of input_ids is 158, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "487ebdc263ea2726"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
